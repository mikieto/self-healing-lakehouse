# ================================================
# Bootstrap-Integrated Official Modules Approach
# ================================================
# terraform/environments/dev/main.tf

terraform {
  required_version = ">= 1.7"
  
  # S3 Backend configuration from bootstrap
  backend "s3" {
    # Configuration provided via backend.hcl from bootstrap
    # Generated by: terraform/bootstrap outputs
  }

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    random = {
      source  = "hashicorp/random"
      version = "~> 3.1"
    }
  }
}

provider "aws" {
  region = var.aws_region
  
  default_tags {
    tags = local.common_tags
  }
}

# ================================================
# DATA SOURCES
# ================================================
data "aws_caller_identity" "current" {}
data "aws_region" "current" {}
data "aws_availability_zones" "available" {
  state = "available"
}

# Random suffix for unique naming (separate from bootstrap)
resource "random_id" "suffix" {
  byte_length = 4
}

# ================================================
# NETWORKING - Official VPC Module
# ================================================
module "vpc" {
  source  = "terraform-aws-modules/vpc/aws"
  version = "~> 5.0"

  name = "${var.project_name}-vpc-${random_id.suffix.hex}"
  cidr = var.networking_config.vpc_cidr

  azs              = slice(data.aws_availability_zones.available.names, 0, var.networking_config.availability_zones)
  private_subnets  = [for k, v in slice(data.aws_availability_zones.available.names, 0, var.networking_config.availability_zones) : cidrsubnet(var.networking_config.vpc_cidr, 8, k + 1)]
  public_subnets   = [for k, v in slice(data.aws_availability_zones.available.names, 0, var.networking_config.availability_zones) : cidrsubnet(var.networking_config.vpc_cidr, 8, k + 101)]
  database_subnets = var.aws_services.enable_rds ? [for k, v in slice(data.aws_availability_zones.available.names, 0, var.networking_config.availability_zones) : cidrsubnet(var.networking_config.vpc_cidr, 8, k + 201)] : []

  enable_nat_gateway = var.networking_config.enable_nat_gateway
  enable_flow_log    = var.networking_config.enable_flow_logs
  
  create_database_subnet_group = var.aws_services.enable_rds

  # VPC Flow Logs configuration
  flow_log_destination_type                = "cloud-watch-logs"
  create_flow_log_cloudwatch_iam_role     = var.networking_config.enable_flow_logs
  create_flow_log_cloudwatch_log_group    = var.networking_config.enable_flow_logs

  tags = local.common_tags
}

# ================================================
# DATA LAKE - Official S3 Module
# ================================================
module "data_lake" {
  source  = "terraform-aws-modules/s3-bucket/aws"
  version = "~> 4.0"

  bucket = "${var.project_name}-data-${random_id.suffix.hex}"

  # Learner-configurable features
  versioning = var.data_lake_config.enable_versioning ? {
    enabled = true
  } : {}

  lifecycle_rule = var.data_lake_config.enable_lifecycle ? [
    {
      id     = "data_lifecycle"
      status = "Enabled"
      transition = [
        {
          days          = 30
          storage_class = "STANDARD_IA"
        },
        {
          days          = 90
          storage_class = "GLACIER"
        }
      ]
    }
  ] : []

  server_side_encryption_configuration = var.data_lake_config.enable_encryption ? {
    rule = {
      apply_server_side_encryption_by_default = {
        sse_algorithm = "AES256"
      }
    }
  } : {}

  # Block public access for security
  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true

  tags = local.common_tags
}

# S3 Event Notifications for self-healing
resource "aws_s3_bucket_notification" "data_events" {
  bucket      = module.data_lake.s3_bucket_id
  eventbridge = true

  depends_on = [module.data_lake]
}

# ================================================
# DATABASE - Official RDS Module (Optional)
# ================================================
module "database" {
  count   = var.aws_services.enable_rds ? 1 : 0
  source  = "terraform-aws-modules/rds/aws"
  version = "~> 6.0"

  identifier = "${var.project_name}-db-${random_id.suffix.hex}"

  engine               = "postgres"
  engine_version       = var.rds_config.engine_version
  family               = "postgres15"
  major_engine_version = "15"
  instance_class       = var.rds_config.instance_class

  allocated_storage     = var.rds_config.allocated_storage
  max_allocated_storage = var.rds_config.allocated_storage * 2
  storage_encrypted     = true

  db_name                     = "lakehouse"
  username                    = "dbadmin"
  manage_master_user_password = true

  vpc_security_group_ids = [aws_security_group.database[0].id]
  db_subnet_group_name   = module.vpc.database_subnet_group
  publicly_accessible    = false
  
  backup_retention_period = var.rds_config.backup_retention
  backup_window          = "03:00-04:00"
  maintenance_window     = "sun:04:00-sun:05:00"
  multi_az              = var.rds_config.multi_az

  # Performance Insights
  performance_insights_enabled = true
  monitoring_interval         = 60
  create_monitoring_role      = true
  monitoring_role_name        = "${var.project_name}-rds-monitoring-${random_id.suffix.hex}"

  # Environment-based deletion protection
  deletion_protection = var.environment == "prod"
  skip_final_snapshot = var.environment != "prod"

  tags = local.common_tags
}

# ================================================
# OBSERVABILITY - Official Grafana Module (Optional)
# ================================================
module "grafana" {
  count   = var.aws_services.enable_grafana ? 1 : 0
  source  = "terraform-aws-modules/managed-service-grafana/aws"
  version = "~> 2.0"

  name        = "${var.project_name}-grafana-${random_id.suffix.hex}"
  description = "Self-Healing Lakehouse Observability Dashboard"

  # Essential configuration
  account_access_type      = "CURRENT_ACCOUNT"
  authentication_providers = ["SAML"]
  permission_type          = "SERVICE_MANAGED"

  data_sources = ["CLOUDWATCH"]
  
  tags = local.common_tags
}

# ================================================
# NOTIFICATIONS - Official SNS Module (Optional)
# ================================================
module "notifications" {
  count   = var.aws_services.enable_sns ? 1 : 0
  source  = "terraform-aws-modules/sns/aws"
  version = "~> 6.0"

  name = "${var.project_name}-alerts-${random_id.suffix.hex}"

  subscriptions = {
    email = {
      protocol = "email"
      endpoint = var.self_healing_config.notification_email
    }
  }

  tags = local.common_tags
}

# ================================================
# AUTOMATION - Official EventBridge Module (Optional)
# ================================================
module "automation" {
  count   = var.aws_services.enable_eventbridge ? 1 : 0
  source  = "terraform-aws-modules/eventbridge/aws"
  version = "~> 3.0"

  create_bus = false # Use default event bus

  rules = {
    s3_data_uploaded = {
      description = "Detect new data files for quality processing"
      event_pattern = jsonencode({
        source      = ["aws.s3"]
        detail-type = ["Object Created"]
        detail = {
          bucket = {
            name = [module.data_lake.s3_bucket_id]
          }
          object = {
            key = [{
              prefix = "raw/"
            }]
          }
        }
      })
      enabled = true
    }

    data_quality_failure = {
      description = "Detect data quality job failures"
      event_pattern = jsonencode({
        source      = ["aws.glue"]
        detail-type = ["Glue Job State Change"]
        detail = {
          state = ["FAILED"]
          jobName = var.processing_config.enable_data_quality ? [aws_glue_job.data_quality[0].name] : []
        }
      })
      enabled = var.processing_config.enable_data_quality
    }
  }

  targets = {
    s3_data_uploaded = [
      {
        name = "NotifyDataUpload"
        arn  = module.notifications[0].topic_arn
        input_transformer = {
          input_paths = {
            bucket = "$.detail.bucket.name"
            key    = "$.detail.object.key"
          }
          input_template = jsonencode({
            alert_type = "new_data_detected"
            message    = "üìÅ New data uploaded: <key> - Starting quality checks..."
            bucket     = "<bucket>"
            object_key = "<key>"
          })
        }
      }
    ]

    data_quality_failure = var.processing_config.enable_data_quality ? [
      {
        name = "NotifyQualityFailure"
        arn  = module.notifications[0].topic_arn
        input_transformer = {
          input_paths = {
            jobName = "$.detail.jobName"
            state   = "$.detail.state"
          }
          input_template = jsonencode({
            alert_type = "data_quality_failure"
            message    = "üö® Data quality job <jobName> failed - Initiating remediation..."
            job_name   = "<jobName>"
            status     = "<state>"
          })
        }
      }
    ] : []
  }

  tags = local.common_tags
}

# ================================================
# MINIMAL CUSTOM RESOURCES
# (Only where official modules don't exist)
# ================================================

# Glue Catalog Database
resource "aws_glue_catalog_database" "main" {
  count = var.processing_config.enable_crawler ? 1 : 0
  name  = "${var.project_name}-catalog-${random_id.suffix.hex}"

  create_table_default_permission {
    permissions = ["SELECT"]
    principal {
      data_lake_principal_identifier = "IAM_ALLOWED_PRINCIPALS"
    }
  }

  tags = local.common_tags
}

# Glue Crawler
resource "aws_glue_crawler" "main" {
  count         = var.processing_config.enable_crawler ? 1 : 0
  name          = "${var.project_name}-crawler-${random_id.suffix.hex}"
  database_name = aws_glue_catalog_database.main[0].name
  role          = aws_iam_role.glue[0].arn

  s3_target {
    path = "s3://${module.data_lake.s3_bucket_id}/raw/"
    exclusions = [
      "**/_temporary/**",
      "**/.spark-staging/**"
    ]
  }

  schema_change_policy {
    update_behavior = "UPDATE_IN_DATABASE"
    delete_behavior = "LOG"
  }

  configuration = jsonencode({
    "Version" = 1.0
    "CrawlerOutput" = {
      "Partitions" = {
        "AddOrUpdateBehavior" = "InheritFromTable"
      }
    }
    "Grouping" = {
      "TableGroupingPolicy" = "CombineCompatibleSchemas"
    }
  })

  schedule = var.processing_config.schedule_crawler

  tags = local.common_tags
}

# Glue Data Quality Job
resource "aws_glue_job" "data_quality" {
  count        = var.processing_config.enable_data_quality ? 1 : 0
  name         = "${var.project_name}-quality-${random_id.suffix.hex}"
  role_arn     = aws_iam_role.glue[0].arn
  glue_version = var.processing_config.glue_version

  command {
    script_location = "s3://${module.data_lake.s3_bucket_id}/scripts/data_quality.py"
    python_version  = "3"
  }

  default_arguments = {
    "--job-language"                     = "python"
    "--job-bookmark-option"              = "job-bookmark-enable"
    "--enable-metrics"                   = "true"
    "--enable-continuous-cloudwatch-log" = "true"
    "--TempDir"                          = "s3://${module.data_lake.s3_bucket_id}/temp/"
    "--source-path"                      = "s3://${module.data_lake.s3_bucket_id}/raw/"
    "--quarantine-path"                  = "s3://${module.data_lake.s3_bucket_id}/quarantine/"
    "--database-name"                    = aws_glue_catalog_database.main[0].name
    "--sns-topic-arn"                    = try(module.notifications[0].topic_arn, "")
  }

  execution_property {
    max_concurrent_runs = 2
  }

  max_retries       = 2
  timeout          = 60
  worker_type      = var.processing_config.worker_type
  number_of_workers = var.processing_config.number_of_workers

  tags = merge(local.common_tags, {
    JobType = "data-quality"
  })
}

# Glue Remediation Job (Optional)
resource "aws_glue_job" "remediation" {
  count        = var.processing_config.enable_remediation ? 1 : 0
  name         = "${var.project_name}-remediation-${random_id.suffix.hex}"
  role_arn     = aws_iam_role.glue[0].arn
  glue_version = var.processing_config.glue_version

  command {
    script_location = "s3://${module.data_lake.s3_bucket_id}/scripts/remediation.py"
    python_version  = "3"
  }

  default_arguments = {
    "--job-language"                     = "python"
    "--job-bookmark-option"              = "job-bookmark-disable"
    "--enable-metrics"                   = "true"
    "--enable-continuous-cloudwatch-log" = "true"
    "--TempDir"                          = "s3://${module.data_lake.s3_bucket_id}/temp/"
    "--quarantine-path"                  = "s3://${module.data_lake.s3_bucket_id}/quarantine/"
    "--processed-path"                   = "s3://${module.data_lake.s3_bucket_id}/processed/"
    "--sns-topic-arn"                    = try(module.notifications[0].topic_arn, "")
    "--database-name"                    = try(aws_glue_catalog_database.main[0].name, "")
  }

  execution_property {
    max_concurrent_runs = 1
  }

  max_retries       = 2
  timeout          = 60
  worker_type      = var.processing_config.worker_type
  number_of_workers = var.processing_config.number_of_workers

  tags = merge(local.common_tags, {
    JobType = "remediation"
  })
}

# ================================================
# IAM ROLES AND POLICIES
# ================================================

# Glue Service Role
resource "aws_iam_role" "glue" {
  count = var.processing_config.enable_crawler || var.processing_config.enable_data_quality ? 1 : 0
  name  = "${var.project_name}-glue-${random_id.suffix.hex}"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "glue.amazonaws.com"
        }
      }
    ]
  })

  tags = local.common_tags
}

# Attach AWS managed Glue service role policy
resource "aws_iam_role_policy_attachment" "glue_service" {
  count      = var.processing_config.enable_crawler || var.processing_config.enable_data_quality ? 1 : 0
  role       = aws_iam_role.glue[0].name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole"
}

# Custom S3 access policy for Glue
resource "aws_iam_policy" "glue_s3_access" {
  count = var.processing_config.enable_crawler || var.processing_config.enable_data_quality ? 1 : 0
  name  = "${var.project_name}-glue-s3-${random_id.suffix.hex}"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "s3:GetObject",
          "s3:PutObject",
          "s3:DeleteObject",
          "s3:ListBucket",
          "s3:GetBucketLocation"
        ]
        Resource = [
          module.data_lake.s3_bucket_arn,
          "${module.data_lake.s3_bucket_arn}/*"
        ]
      }
    ]
  })

  tags = local.common_tags
}

resource "aws_iam_role_policy_attachment" "glue_s3" {
  count      = var.processing_config.enable_crawler || var.processing_config.enable_data_quality ? 1 : 0
  role       = aws_iam_role.glue[0].name
  policy_arn = aws_iam_policy.glue_s3_access[0].arn
}

# SNS publish permissions for Glue
resource "aws_iam_policy" "glue_sns_access" {
  count = var.aws_services.enable_sns && (var.processing_config.enable_data_quality || var.processing_config.enable_remediation) ? 1 : 0
  name  = "${var.project_name}-glue-sns-${random_id.suffix.hex}"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "sns:Publish"
        ]
        Resource = module.notifications[0].topic_arn
      }
    ]
  })

  tags = local.common_tags
}

resource "aws_iam_role_policy_attachment" "glue_sns" {
  count      = var.aws_services.enable_sns && (var.processing_config.enable_data_quality || var.processing_config.enable_remediation) ? 1 : 0
  role       = aws_iam_role.glue[0].name
  policy_arn = aws_iam_policy.glue_sns_access[0].arn
}

# ================================================
# SECURITY GROUPS
# ================================================

# Database Security Group
resource "aws_security_group" "database" {
  count  = var.aws_services.enable_rds ? 1 : 0
  name   = "${var.project_name}-db-sg-${random_id.suffix.hex}"
  vpc_id = module.vpc.vpc_id

  ingress {
    description = "PostgreSQL from VPC"
    from_port   = 5432
    to_port     = 5432
    protocol    = "tcp"
    cidr_blocks = [module.vpc.vpc_cidr_block]
  }

  egress {
    description = "All outbound traffic"
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = merge(local.common_tags, {
    Name = "${var.project_name}-database-sg"
  })
}

# ================================================
# CLOUDWATCH DASHBOARDS
# ================================================

# Main Self-Healing Dashboard
resource "aws_cloudwatch_dashboard" "self_healing" {
  count          = var.aws_services.enable_cloudwatch ? 1 : 0
  dashboard_name = "${var.project_name}-dashboard-${random_id.suffix.hex}"

  dashboard_body = jsonencode({
    widgets = [
      {
        type   = "metric"
        x      = 0
        y      = 0
        width  = 12
        height = 6
        properties = {
          metrics = [
            ["AWS/S3", "NumberOfObjects", "BucketName", module.data_lake.s3_bucket_id, "StorageType", "AllStorageTypes"]
          ]
          period = 300
          stat   = "Average"
          region = var.aws_region
          title  = "üìä Data Lake Objects Count"
          yAxis = {
            left = {
              min = 0
            }
          }
        }
      },
      {
        type   = "metric"
        x      = 0
        y      = 6
        width  = 12
        height = 6
        properties = {
          metrics = var.aws_services.enable_cloudwatch ? [
            ["AWS/Events", "MatchedEvents", "RuleName", "s3_data_uploaded"]
          ] : []
          period = 300
          stat   = "Sum"
          region = var.aws_region
          title  = "‚ö° Self-Healing Events Triggered"
        }
      }
    ]
  })
}

# ================================================
# LOCALS
# ================================================
locals {
  common_tags = {
    Project     = var.project_name
    Environment = var.environment
    ManagedBy   = "terraform"
    Purpose     = "self-healing-lakehouse"
    Approach    = "official-modules-first"
    Bootstrap   = "s3-native-locking"
  }
}